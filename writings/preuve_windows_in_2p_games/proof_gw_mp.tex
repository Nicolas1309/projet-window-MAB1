\documentclass{article}

\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{url}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{amsmath, amssymb, amsthm, verbatim}
\usepackage{IEEEtrantools}
\usepackage[margin=1in]{geometry}
\usepackage[colorlinks, linkcolor=blue]{hyperref}
\usepackage{epigraph}
\usepackage{mathrsfs}
\usepackage[toc,page]{appendix}
\usepackage{xcolor}
\usepackage{enumitem}

%Pour Ã©crire des algos:
\usepackage{algorithmic, algorithm}
\usepackage{float}

\newcommand{\IR}{\mathbb{R}}
\newcommand{\IN}{\mathbb{N}}
\theoremstyle{plain}
\newtheorem{thm}{Theorem}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{corollary}[thm]{Corollary}
\newtheorem{rem}{Remark}

\title{Algorithm for good window mean-payoff in two players games}
\author{Nicolas Lecomte}
\date{\today}

\begin{document}

\maketitle

We fix a two-player game $G = (S_1, S_2, E, w)$ and $l_{max} \in \IN$.

Now, we can define the following:

\[\forall s \in S, C_0(s)=0 \wedge \forall 1 \leqslant i \leqslant l_{max}, C_i(s) =
\begin{cases}
\max_{(s, s') \in E} \{ \min \{w(s, s'), w(s, s') + C_{i-1}(s')\} \} & \text{if } s \in S_1,\\
\min_{(s, s') \in E} \{ \max \{w(s, s'), w(s, s') + C_{i-1}(s')\} \} & \text{if } s \in S_2.
\end{cases}
\]

The main idea behind this definition is that the value $C_i(s)$ denotes the best sum that $\mathcal{P}_1$ can force in at most $i$ steps from the state $s$ in $G$. In other words, $C_i(s)$ is the smallest sum that $\mathcal{P}_1$ can compensate in $i$ steps. The following lemma formalizes this intuition.

\begin{lem} \label{lem:C_i}
Let $1 \leqslant i \leqslant l_{max}$ and $s \in S$, we have:

\[ C_i(s) = \max_{\sigma_1 \in \Sigma_1} \min_{\sigma_2 \in \Sigma_2} BestSum_i(Outcome_G(s, \sigma_1, \sigma_2)) \]

where for all $\pi \in Plays(G), BestSum_i(\pi) = \max \{n \in \IN | \exists j \leqslant i, \sum_{k=0}^{j-1} w(e_\pi (k, k+1)) \geqslant n \}$. So, $BestSum_i(\pi)$ is the best sum on the weights of the edges along a play $\pi$ within $i$ steps.

\end{lem}

\begin{rem}
The maximum and minimum exist because, if we fix $s \in S$ and $1 \leqslant i \leqslant l_{max}$, the supremum and infimum are reached in the formula

$$\sup_{\sigma_1 \in \Sigma_1} \inf_{\sigma_2 \in \Sigma_2} BestSum_i(Outcome_G(s, \sigma_1, \sigma_2))$$

Indeed, one can observe that $BestSum_i$ depends only on the prefixes of length $i$ of $Outcome_G(s, \sigma_1, \sigma_2)$. So, as there is a finite number of prefixes of length $i$ in the game (bounded by $|S|^i$), there is also a finite number of possible values for $BestSum_i$. Thus, the supremum and infimum have to be reached.
\end{rem}

\begin{proof}
We will prove the proposition by induction on $i$. So, let us fix a state $s \in S$.

Basic case: $i = 1$. If $s$ belongs to $S_1$, then $C_1(s) = \max_{(s, s') \in E} w(s, s')$ and, clearly, the best sum that we can see along a play in $1$ step is the largest weight on an edge leaving $s$. Furthermore, this edge can be taken in a strategy of player 1 and the result does not depend on any strategy of player $2$. A symmetrical argument can be used if $s$ belongs to $S_2$.

Induction step: let $1 \leqslant i < l_{max}$ be an integer such that the lemma is true for $i$. We will thus show that it is still true for $i+1$.

For the remaining of this proof, we write $\pi$ instead of $Outcome_G(s, \sigma_1, \sigma_2)$ for $\sigma_1 \in \Sigma_1$ and $\sigma_2 \in \Sigma_2$ for readability. Without loss of generality, we can assume that $s$ belongs to player $1$'s states because the arguments are similar.

First, we can develop $C_{i+1}(s)$, using its definition and the fact that the first vertex of $Outcome_G(s, \sigma_1, \sigma_2)$ is $s$ that belongs to $S_1$. So, we have the following:

\begin{align}
&\max_{\sigma_1 \in \Sigma_1} \min_{\sigma_2 \in \Sigma_2} BestSum_{i+1}(Outcome_G(s, \sigma_1, \sigma_2))
\\
&= \max_{\sigma_1 \in \Sigma_1} \min_{\sigma_2 \in \Sigma_2} \max \bigg\{ \sum_{k=0}^{j-1} w(e_{\pi}(k, k+1)) ~|~ 0 < j \leqslant i+1 \bigg\}
\\
&= \max_{\sigma_1 \in \Sigma_1} \min_{\sigma_2 \in \Sigma_2} \max \bigg\{ w(e_{\pi}(0, 1)) + \sum_{k=1}^{j-1} w(e_{\pi}(k, k+1)) ~|~ 0 < j \leqslant i+1 \bigg\}
\\
&= \max_{\sigma_1 \in \Sigma_1} \min_{\sigma_2 \in \Sigma_2} \max \bigg\{ w(e_{\pi}(0, 1)), \max \{w(e_{\pi}(0, 1)) + \sum_{k=1}^{j-1} w(e_{\pi}(k, k+1)) ~|~ 0 < j \leqslant i+1 \}\bigg\}
\\
&= \max_{\sigma_1 \in \Sigma_1} \min_{\sigma_2 \in \Sigma_2} \max \bigg\{ w(e_{\pi}(0, 1)), w(e_{\pi}(0, 1)) + \max \Big\{\sum_{k=1}^{j-1} w(e_{\pi}(k, k+1)) ~|~ 0 < j \leqslant i+1 \Big\}\bigg\}
\\
&= \max_{\sigma_1 \in \Sigma_1} \min_{\sigma_2 \in \Sigma_2} \max \bigg\{ w(s, \sigma_1(s)), w(s, \sigma_1(s)) + \max \Big\{\sum_{k=1}^{j-1} w(e_{\pi}(k, k+1)) ~|~ 0 < j \leqslant i+1 \Big\}\bigg\}
\end{align}

Now, for a fixed strategy $\sigma$, we define $\tilde{\sigma}$, the corresponding strategy such that, for all $s_1 \dots s_n \in Prefs_1(G)$, $\tilde{\sigma}(s_1 \dots s_n) = \sigma (s s_1 \dots s_n)$. Now, we can rewrite the preceding equality and use the definition of $BestSum_i$.

\begin{align}
&\max_{\sigma_1 \in \Sigma_1} \min_{\sigma_2 \in \Sigma_2} \max \bigg\{ w(s, \sigma_1(s)), w(s, \sigma_1(s)) + \max \Big\{\sum_{k=0}^{j-1} w(e_{Outcome_G(\sigma_1(s), \tilde{\sigma}_1, \tilde{\sigma}_2)}(k, k+1)) ~|~ 0 < j \leqslant i \Big\}\bigg\}
\\
&= \max_{\sigma_1 \in \Sigma_1} \min_{\sigma_2 \in \Sigma_2} \max \{ w(s, \sigma_1(s)), w(s, \sigma_1(s)) + BestSum_i(Outcome_G(\sigma_1(s), \tilde{\sigma}_1, \tilde{\sigma}_2))\}
\end{align}

To conclude the proof, we would want to use the induction hypothesis. To do so, it suffices to inverting some min and max on the last equality as follows:

\begin{align}
&\max_{\sigma_1 \in \Sigma_1} \max \{ w(s, \sigma_1(s)), w(s, \sigma_1(s)) + \min_{\sigma_2 \in \Sigma_2} BestSum_i(Outcome_G(\sigma_1(s), \tilde{\sigma}_1, \tilde{\sigma}_2))\}
\\
&= \max_{(s, s') \in E} \max_{\substack{\sigma_1 \in \Sigma_1 \text{ tq }\\ \sigma_1(s) = s'}} \max \{ w(s, s'), w(s, s') + \min_{\sigma_2 \in \Sigma_2} BestSum_i(Outcome_G(s', \tilde{\sigma}_1, \tilde{\sigma}_2))\}
\\
&= \max_{(s, s') \in E} \max \{ w(s, s'), w(s, s') + \max_{\substack{\sigma_1 \in \Sigma_1 \text{ tq }\\ \sigma_1(s) = s'}} \min_{\sigma_2 \in \Sigma_2} BestSum_i(Outcome_G(s', \tilde{\sigma}_1, \tilde{\sigma}_2))\}
\\
&= \max_{(s, s') \in E} \max \{ w(s, s'), w(s, s') + \max_{\sigma_1 \in \Sigma_1} \min_{\sigma_2 \in \Sigma_2} BestSum_i(Outcome_G(s', \tilde{\sigma}_1, \tilde{\sigma}_2))\}
\\
&= \max_{(s, s') \in E} \max \{ w(s, s'), w(s, s') + \max_{\sigma_1 \in \Sigma_1} \min_{\sigma_2 \in \Sigma_2} BestSum_i(Outcome_G(s', \sigma_1, \sigma_2))\}
\\
&= \max_{(s, s') \in E} \max \{ w(s, s'), w(s, s') + C_i(s')\}
\\
&= C_{i+1}(s)
\end{align}

Finally, the induction hypothesis from the vertex $s'$ is used in the line $(14)$ and the definition of $C_{i+1}(s)$ concludes the proof. 

\end{proof}

\begin{prop}
We have, for all state $s \in S$:
\[C_{l_{max}}(s) \geqslant 0 \iff \text{ s is a winning state in G for $\mathcal{P}_1$ for the winodw objective } GW_{MP}(G, l_{max})\]
\end{prop}

\begin{proof}
Let $s \in S$ be a state of the game,\\
We have the following equivalences:
\begin{align}
C_{l_{max}}(s) \geqslant 0 &\iff max_{\sigma_1 \in \Sigma_1} min_{\sigma_2 \in \Sigma_2} BestSum_{l_{max}}(Outcome_G(s, \sigma_1, \sigma_2)) \geqslant 0\\
						   &\iff \exists \sigma_1 \in \Sigma_1, \forall \sigma_2 \in \Sigma_2, BestSum_{l_{max}}(Outcome_G(s, \sigma_1, \sigma_2)) \geqslant 0\\
						   &\iff \exists \sigma_1 \in \Sigma_1, \forall \sigma_2 \in \Sigma_2, \exists j \leqslant l_{max}, \sum_{k=0}^{l_{max}-1} w(e_{Outcome_G(s, \sigma_1, \sigma_2)} (k, k+1)) \geqslant 0\\
						   &\iff \exists \sigma_1 \in \Sigma_1, \forall \sigma_2 \in \Sigma_2, Outcome_G(s, \sigma_1, \sigma_2) \in GW_{MP}(G, l_{max})\\
						   &\iff \text{ s is a winning state in G for $\mathcal{P}_1$ for the objective } GW_{MP}(G, l_{max})
\end{align}

The first equivalence is given by the lemma \ref{lem:C_i}, the third one is just the definition of $BestSum$ and the fourth one uses the definition of $GW_{MP}(G, l_{max})$.

\end{proof}

\end{document}

















